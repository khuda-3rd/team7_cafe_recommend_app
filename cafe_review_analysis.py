# -*- coding: utf-8 -*-
"""cafe_review_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cnmZ_sQhmEds-OQUHVWYOZdBOWpHJ-Ie

##**기본세팅**
"""

#구글 드라이브와 연동
from google.colab import drive
drive.mount('/content/drive')

#필요한 라이브러리 설치
import pandas as pd
import re

dfList = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13]

"""##**특수기호 제거 함수**"""

#특수기호 제거 함수
def clean_text(text_input):
  text = re.sub(r'[^\uAC00-\uD7A30-9a-zA-Z\s]','',text_input)
  #text = text.replace('\n'," ")
  text = re.sub(r'<[^>]+>','',text) #remove Html tags
  text = re.sub(r'\s+', ' ', text) #remove spaces
  text = re.sub(r"^\s+", '', text) #remove space from start
  text = re.sub(r'\s+$', '', text) #remove space from the end
  return text

"""##**데이터 자연어처리**"""

!pip install transformers

# import library
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline

# load model
model_name = "jaehyeong/koelectra-base-v3-generalized-sentiment-analysis"
tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=512, truncation=True)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
sentiment_classifier = TextClassificationPipeline(tokenizer=tokenizer, model=model)

#데이터 로드
js = pd.read_json('/content/drive/MyDrive/KHUDA/KHUDA_3기/3차프로젝트/예지/data/cafeList.json')
cafe_df = pd.DataFrame(js)

#특수기호 제거
reviewList = cafe_df['review']
for i, sublist in enumerate(reviewList):
    for j, review in enumerate(sublist):
        reviewList[i][j] = clean_text(review)

#위도/경도 추출
from geopy.geocoders import Nominatim
geo_local = Nominatim(user_agent='South Korea')

minus4 = [2,3,13,16,19,26,30,72,79,84,85,92]
minus5 = [68]
minus6 = [36,42]
minus7 = [28,34,53,76]
minus9 = [11,89]
minus10 = [19]
minus13 = [5,12,44,63,66]
minus14 = [87,94]
minus15 = [93]
minus16 = [64]

#도로명주소 추출
def doroAddress(df):
  address_pre = df['address']
  for idx,address in enumerate(address_pre):
    address = address.split('\n')[0]
    if 'F' in address:
      f_index = address.find('F')  # 'F' 문자의 인덱스를 찾음
      address = address[:f_index-2]
      if idx in minus4:
        address = address[:-4]
      elif idx in minus5:
        address = address[:-5]
      elif idx in minus6:
        address = address[:-6]
      elif idx in minus7:
        address = address[:-7]
      elif idx in minus9:
        address = address[:-9]
      elif idx in minus13:
        address = address[:-13]
      elif idx in minus14:
        address = address[:-14]
      elif idx in minus15:
        address = address[:-15]
      elif idx in minus16:
        address = address[:-16]
    elif idx in minus4:
      address = address[:-4]
    elif idx in minus5:
      address = address[:-5]
    elif idx in minus6:
      address = address[:-6]
    elif idx in minus7:
      address = address[:-7]
    elif idx in minus9:
      address = address[:-9]
    elif idx in minus13:
      address = address[:-13]
    elif idx in minus14:
      address = address[:-14]
    elif idx in minus15:
      address = address[:-15]
    elif idx in minus16:
      address = address[:-16]
    address_pre[idx] = address

# 위도, 경도 반환하는 함수
def geocoding(df):
  df['lat'] = None
  df['lng'] = None
  for i, address in enumerate(df['address']):
    try:
      geo = geo_local.geocode(address)
      x_y = [geo.latitude, geo.longitude]
      df['lat'][i] = x_y[0]
      df['lng'][i] = x_y[1]
    except:
      df['lat'][i] = 0
      df['lng'][i] = 0

doroAddress(cafe_df)

geocoding(cafe_df)

#잘못된 위도/경도값 추출하여 확인
cafe_df[cafe_df['lat']==0]

#긍정리뷰 비율로 카페 점수 내기
def trim_review(review):
    max_length = 512
    if len(review) <= max_length:
        return review
    else:
        return review[:max_length]

def cafe_score(df):
  df['positiveReviewRatio'] = None
  reviewRatio = []
  df_review = df['review']

  for i, dummy in enumerate(df_review):
    review_score = 0
    name = df['name'][i]

    for review in dummy:
      trimmed_review = trim_review(review)
      pred = sentiment_classifier(trimmed_review)
      label = pred[0]['label']
      score = pred[0]['score']

      if (label=='1'):
        review_score += score
      else:
        scalingScore = 1-score
        review_score += scalingScore

    df_score = review_score/len(dummy) # 해당 카페의 모든 리뷰에 대한 평균 계산
    reviewRatio.append(df_score)
    print(f'{name} >> {df_score}')
  df['positiveReviewRatio'] = reviewRatio

cafe_score(cafe_df)

"""##**키워드 추출**"""

!pip install keybert

from keybert import KeyBERT
k_model = KeyBERT('distilbert-base-nli-mean-tokens')

#  SBERT 를 위한 패키지인 sentence_transformers, 형태소 분석기 KoNLPy 설치
!pip install sentence_transformers
!pip install konlpy

import numpy as np
import itertools
from konlpy.tag import Okt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# 한국어를 포함하고 있는 다국어 SBERT 를 로드
model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')

def keyword_func(doc, top_n, nr_candidates):
  # 형태소 분석기를 통해 명사만 추출한 문서 만들기
  okt = Okt()
  tokenized_doc = okt.pos(doc)
  tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])

  # 사이킷런의 CountVectorizer를 사용해 단어 추출 / 이유: n_gram_range의 인자를 사용하면 쉽게 n-gram 추출이 가능하기 때문
  n_gram_range = (1, 2) # 결과 후보는 2 개의 단어를 한 묶음으로 간주하는 bigram 과 3 개의 단어를 한 묶음으로 간주하는 trigram 을 추출
  count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])
  candidates = count.get_feature_names_out()
  doc_embedding = model.encode([doc]) # 문서 encoding하기
  candidate_embeddings = model.encode(candidates) # 추출된 단어 인코딩하기

  # 문서와 각 키워드들 간의 유사도
  distances = cosine_similarity(doc_embedding, candidate_embeddings)

  # 각 키워드들 간의 유사도
  distances_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)

  # 코사인유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.
  words_idx = list(distances.argsort()[0][-nr_candidates:])
  words_vals = [candidates[index] for index in words_idx]
  distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]

  # 각 키워드들중에서 가장 덜 유사한 키워드들 간의 조합을 계산 (# 상위 10 개의 키워드를 선택하고 이 10 개 중에서 서로 가장 유사성이 낮은 5 개를 선택)
  min_sim = np.inf

  candidate = None
  for combination in itertools.combinations(range(len(words_idx)), top_n):
    sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])
    if sim < min_sim:
      candidate = combination
      min_sim = sim
  blog_keyword = [words_vals[idx] for idx in candidate]

  return blog_keyword

keywords_list = []
data = cafe_df['review']

for i in range(len(data)):
  name = cafe_df['name'][i]
  review_dummy = ' '.join(cafe_df['review'][i])
  result = keyword_func(review_dummy, top_n=3, nr_candidates=24)
  keywords_list.append(result)
  print(f'{name}의 키워드: {result}')

"""##**파일 저장**"""

new_df = pd.DataFrame({'keywords': keywords_list})
cafe_df = cafe_df[['name', 'address', 'positiveReviewRatio','photoUrlList','lat','lng']]
# 인덱스를 기준으로 두 데이터프레임 합치기
df_final = pd.concat([cafe_df,new_df],axis=1)

#데이터프레임 정리
df_final = df_final[['name', 'address', 'keywords', 'positiveReviewRatio','photoUrlList','lat','lng']]

#json파일로 저장 !photoUrlList 5개!
import json

# Save the DataFrame to a JSON file with 'orient' set to 'table'
df_final.to_json('df_to_json5.json', orient='table', force_ascii=False, indent=4, index=False)

# Read the JSON file
with open('df_to_json5.json', 'r', encoding='utf-8') as file:
    json_data = json.load(file)

# Extract the 'data' part from the JSON
data_only = json_data['data']

# Save the 'data' part to a new JSON file
with open('data_final5.json', 'w', encoding='utf-8') as file:
    json.dump(data_only, file, indent=4, ensure_ascii=False)

